results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
source("~/Desktop/NIR/code/spectra analysis.R")
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, data.analysis$class, 6)
if (!file.exists("data/results/results2.rds")) {
feature_analysis(task, resampling_instance = resampling.instance, filter = FALSE, pca = TRUE)
}
results2 <- readRDS("data/results/results2.rds")
results2$performance
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$benchmark_obj
library(mlr3viz)
autoplot(results$benchmark_obj, measure = msr("classif.acc")) +
theme_minimal() +
labs(title = "Stabilità dell'Accuracy tra i 5 Fold")
library(mlr3viz)
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
theme_minimal() +
labs(title = "Stabilità dell'Accuracy tra i 5 Fold")
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "Confronto Curve ROC (Media dei Fold)")
install.packages("precrec")
source("~/Desktop/NIR/settings.R")
autoplot(results$benchmark_obj, type = "roc") +
labs(title = "Confronto Curve ROC (Media dei Fold)")
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "Confronto Curve ROC (Media dei Fold)")
install.packages("mlr3benchmark")
library(mlr3benchmark)
# install.packages("mlr3benchmark")
library(mlr3benchmark)
# Trasformiamo l'oggetto benchmark di mlr3 in un oggetto mlr3benchmark
bmr_analysis = as_benchmark_aggr(results2$benchmark_obj, measures = msr("classif.acc"))
# Eseguiamo il test di Friedman
friedman_test = bmr_analysis$friedman_test()
library(mlr3viz)
autoplot(results$benchmark_obj, measure = msr("classif.acc")) +
geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") + # Linea del caso (50%)
theme_minimal()
library(mlr3viz)
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") + # Linea del caso (50%)
theme_minimal()
# Creiamo l'oggetto aggregato senza mediare i fold
# (Usiamo l'ID del fold come elemento di distinzione)
bmr_analysis = as_benchmark_aggr(results$benchmark_obj)
# Creiamo l'oggetto aggregato senza mediare i fold
# (Usiamo l'ID del fold come elemento di distinzione)
bmr_analysis = as_benchmark_aggr(results2$benchmark_obj)
# Estraiamo le performance di ogni singolo fold
perf_data = results$benchmark_obj$score(msr("classif.acc"))
# Estraiamo le performance di ogni singolo fold
perf_data = results2$benchmark_obj$score(msr("classif.acc"))
# Test di Kruskal-Wallis (alternativa non parametrica a Friedman per campioni indipendenti)
kruskal_res = kruskal.test(classif.acc ~ learner_id, data = perf_data)
print(kruskal_res)
# Per vedere i confronti a coppie (Post-hoc)
pairwise.wilcox.test(perf_data$classif.acc, perf_data$learner_id, p.adjust.method = "bonferroni")
# Trasformiamo l'oggetto benchmark di mlr3 in un oggetto mlr3benchmark
bmr_analysis = as_benchmark_aggr(results$benchmark_obj, measures = msr("classif.auc"))
# Trasformiamo l'oggetto benchmark di mlr3 in un oggetto mlr3benchmark
bmr_analysis = as_benchmark_aggr(results2$benchmark_obj, measures = msr("classif.auc"))
autoplot(results$benchmark_obj, measure = msr("classif.auc")) +
theme_minimal() +
labs(title = "Stabilità dell'Accuracy tra i 5 Fold")
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
theme_minimal() +
labs(title = "Stabilità dell'Accuracy tra i 5 Fold")
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
theme_minimal() +
labs(title = "Stabilità dell'Accuracy tra i 5 Fold")
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "Confronto Curve ROC (Media dei Fold)")
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
theme_minimal() +
labs(title = "Stabilità dell'AUC tra i 5 Fold")
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
theme_minimal() +
labs(title = "Stabilità dell'Accuracy tra i 5 Fold")
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "Confronto Curve ROC (Media dei Fold)")
pattern <- "results([0-9]+)\\.(([0-9])+\\.)?rds"
string <- "results1.1.rds"
regmatches(string, regexec(pattern, string, perl = TRUE)
fd
regmatches(string, regexec(pattern, stirng, perl = TRUE))
regmatches(string, regexec(pattern, string, perl = TRUE))
string <- "results1.png"
regmatches(string, regexec(pattern, string, perl = TRUE))
string <- "results1.rds"
regmatches(string, regexec(pattern, string, perl = TRUE))
pattern <- "results([0-9]+)(\\.([0-9])+)?\\.rds"
regmatches(string, regexec(pattern, string, perl = TRUE))
string <- "results1.1.rds"
regmatches(string, regexec(pattern, string, perl = TRUE))
x <- string
as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]][[c(2, 3)]])
as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]][[2:3]])
as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]][[2, 3]])
a <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
a[2,3]
a[c(2,3)]
pattern <- "results([0-9]+)\\.(([0-9])+\\.)?rds"
a <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
a[c(2,3)]
regmatches(x, regexec(pattern, x, perl = TRUE))
existing.files <- string
pattern <- "results([0-9]+)\\.(([0-9])+\\.)?rds"
values <- vapply(existing.files, function(x) {
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
for (i in seq_len(values.selected)) {
if (is.na(values.selected[[i]])) values.selected[[i]] <- 0
}
}, numeric(2))
values <- vapply(existing.files, function(x) {
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
for (i in seq_len(values.selected)) {
if (is.na(values.selected[[i]])) values.selected[[i]] <- 0
}
values.selected
}, numeric(2))
values
values.selected
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
for (i in seq_len(values.selected)) {
if (is.na(values.selected[[i]])) values.selected[[i]] <- 0
}
values.selected
values <- vapply(existing.files, function(x) {
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
for (i in seq_len(values.selected)) {
if (is.na(values.selected[[i]])) values.selected[[i]] <- 0
}
values.selected
}, numeric(2))
unname(values)
class(values)
t(values)
values <- vapply(existing.files, function(x) {
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
for (i in seq_len(values.selected)) {
if (is.na(values.selected[[i]])) values.selected[[i]] <- 0
}
values.selected
}, vector(mode = "numeric", 2))
values
print(paste0("data/results/results", max(values[[1]]), ".", max(values[[2]]) + 1, ".rds"))
existing.files <- "results1.rds"
values <- vapply(existing.files, function(x) {
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
for (i in seq_len(values.selected)) {
if (is.na(values.selected[[i]])) values.selected[[i]] <- 0
}
values.selected
}, vector(mode = "numeric", 2))
values
x <- existing.files
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
for (i in seq_len(values.selected)) {
if (is.na(values.selected[[i]])) values.selected[[i]] <- 0
}
values.selected
values.selected
seq_len(values.selected)
values.selected[which(is.na(values.selected))] <- 0
values.selected
values <- vapply(existing.files, function(x) {
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
values.selected[which(is.na(values.selected))] <- 0
}, vector(mode = "numeric", 2))
values <- vapply(existing.files, function(x) {
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
values.selected[which(is.na(values.selected))] <- 0
values.selected
}, vector(mode = "numeric", 2))
print(paste0("data/results/results", max(values[[1]]), ".", max(values[[2]]) + 1, ".rds"))
existing.files <- "data/results/results1.1.rds"
values <- vapply(existing.files, function(x) {
values <- as.numeric(regmatches(x, regexec(pattern, x, perl = TRUE))[[1]])
values.selected <- values[c(2, 3)]
values.selected[which(is.na(values.selected))] <- 0
values.selected
}, vector(mode = "numeric", 2))
print(paste0("data/results/results", max(values[[1]]), ".", max(values[[2]]) + 1, ".rds"))
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, data.analysis$class, 6)
if (!file.exists("data/results/results2.rds")) {
feature_analysis(task, resampling_instance = resampling.instance, filter = FALSE, pca = TRUE)
}
results2 <- readRDS("data/results/results2.rds")
results2$performance
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "Stabilità dell'AUC tra i 5 Fold") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Stabilità dell'Accuracy tra i 5 Fold") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "Confronto Curve ROC (Media dei Fold)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
library(utils)
library(purrr)
# 1. Definiamo il pool di componenti basandoci sull'evidenza dei tuoi modelli precendenti
pc_pool <- paste0("PC", 1:8)
# 2. Generiamo tutte le combinazioni da 2 a 6 elementi
# Questo produrrà esattamente 238 combinazioni
min_pc <- 2
max_pc <- 6
combinations_list <- map(min_pc:max_pc, function(k) {
# Genera le combinazioni e le trasforma in una lista di vettori di caratteri
combs <- combn(pc_pool, k, simplify = FALSE)
return(combs)
}) %>% flatten()
# 3. Verifica della lunghezza della lista
print(paste("Totale combinazioni da testare:", length(combinations_list)))
source("~/Desktop/NIR/code/QDA deep.R")
source("settings.R")
source("source_all.R")
if (!file.exists("data/intermediate/data analysis.rds")) {
build_data_main()
}
data.analysis <- readRDS("data/intermediate/data analysis.rds")
knitr::include_graphics("plot/densities1.png")
knitr::include_graphics("plot/densities2.png")
data.imputed <- readRDS("data/intermediate/data imputated.rds")
knitr::include_graphics("plot/imputation_7_K value.png")
data.cluster <- data.analysis |> dplyr::select(grep("^[[:digit:]]*$", colnames(data.analysis), value = TRUE))
colnames(data.cluster) <- paste0("V", colnames(data.cluster))
data.cluster.smooth <- smoothing(data.cluster)
data.cluster.snv <- as_tibble(t(apply(data.cluster.smooth, MARGIN = 1, FUN = normalization)))
plot_spectral_comparison(data.cluster, data.cluster.snv, n_samples = 6)
knitr::include_graphics("plot/spectral smoothing comparison.png")
data.cluster.final <- bind_cols(data.cluster.snv, class = data.analysis$class)
cor.data <- data.analysis |> dplyr::select(!c(sample, class))
corr_Kvalue <- cor(cor.data)["K value",]
head(sort(abs(corr_Kvalue), decreasing = TRUE), 10)
task <- TaskClassif$new(
id = "Classification original data",
backend = data.cluster.final,
target = "class",
positive = "fresh"
)
task$col_roles$stratum <- "class"
methods <- c("anova", "auc", "mrmr")
tab <- as.data.frame(lapply(methods, function(i) feature_selection(task, i)))
colnames(tab) <- methods
head(tab, 10)
set.seed(123)
resampling.outer <- rsmp("cv", folds = 5)
resampling.instance <- resampling.outer$instantiate(task)
if (!file.exists("data/results/results1.rds")) {
feature_analysis(task, resampling_instance = resampling.instance, filter_type = "mrmr")
}
results1 <- readRDS("data/results/results1.rds")
results1$performance
autoplot(results1$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "Stabilità dell'AUC tra i 5 Fold") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, data.analysis$class, 6)
if (!file.exists("data/results/results2.rds")) {
feature_analysis(task, resampling_instance = resampling.instance, filter = FALSE, pca = TRUE)
}
results2 <- readRDS("data/results/results2.rds")
results2$performance
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "Stabilità dell'AUC tra i 5 Fold") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Stabilità dell'Accuracy tra i 5 Fold") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "Confronto Curve ROC (Media dei Fold)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
library(utils)
library(purrr)
# 1. Definiamo il pool di componenti basandoci sull'evidenza dei tuoi modelli precendenti
pc_pool <- paste0("PC", 1:8)
# 2. Generiamo tutte le combinazioni da 2 a 6 elementi
# Questo produrrà esattamente 238 combinazioni
min_pc <- 2
max_pc <- 6
combinations_list <- map(min_pc:max_pc, function(k) {
# Genera le combinazioni e le trasforma in una lista di vettori di caratteri
combs <- combn(pc_pool, k, simplify = FALSE)
return(combs)
}) %>% flatten()
# 3. Verifica della lunghezza della lista
print(paste("Totale combinazioni da testare:", length(combinations_list)))
results_list <- lapply(combinations_list, function(x) {
model_qda_deep_dive(task, resampling_instance =  resampling.instance, pca_fix = TRUE, pca_selected = x)
})
results_list
# 1. Funzione per estrarre i dati da ogni elemento della lista prodotta da lapply
extract_metrics <- function(res_item, index) {
# Assumiamo che res_item sia la lista che restituisci:
# list(performance = ..., confusion = ..., resampling_results = ..., final_model_object = ...)
if (is.null(res_item)) return(NULL)
# Estraiamo le PC usate (che avevamo salvato o che sono nel nome della combinazione)
# Se hai salvato la combinazione nel modello o nella lista, recuperala qui
# In questo esempio usiamo l'indice della lista combinations_list originale
current_pcs <- paste(combinations_list[[index]], collapse = ", ")
data.frame(
rank = index,
pcs = current_pcs,
acc = res_item$performance["classif.acc"],
auc = res_item$performance["classif.auc"],
sens = res_item$performance["classif.sensitivity"],
spec = res_item$performance["classif.specificity"],
stringsAsFactors = FALSE
)
}
# 2. Applichiamo la funzione a tutta la lista dei risultati
#results_lapply è il nome della lista ottenuta dal tuo lapply(combinations_list, ...)
summary_df <- imap_dfr(results_lapply, extract_metrics)
# 2. Applichiamo la funzione a tutta la lista dei risultati
#results_lapply è il nome della lista ottenuta dal tuo lapply(combinations_list, ...)
summary_df <- imap_dfr(results_list, extract_metrics)
# 3. Creiamo la classifica delle Top 10
# Ordiniamo per AUC (la metrica più robusta) e poi per Sensitivity
top_10_results <- summary_df %>%
arrange(desc(auc), desc(sens)) %>%
head(10)
# 4. Visualizziamo il risultato
print(top_10_results)
result_final_fix <- model_qda_deep_dive(task, resampling_instance = resampling.instance, pca_rank = 12, pca_fix = TRUE, pca_selected = c("PC1","PC2", "PC3", "PC5"))
result_final_fix$performance
# 3. Creiamo la classifica delle Top 10
# Ordiniamo per AUC (la metrica più robusta) e poi per Sensitivity
top_10_results <- summary_df %>%
arrange(desc(acc), desc(sens)) %>%
head(10)
# 4. Visualizziamo il risultato
print(top_10_results)
# 3. Creiamo la classifica delle Top 10
# Ordiniamo per AUC (la metrica più robusta) e poi per Sensitivity
top_10_results <- summary_df %>%
arrange(desc(spec), desc(sens)) %>%
head(10)
# 4. Visualizziamo il risultato
print(top_10_results)
# 3. Creiamo la classifica delle Top 10
# Ordiniamo per AUC (la metrica più robusta) e poi per Sensitivity
top_10_results <- summary_df %>%
arrange(desc(auc), desc(sens)) %>%
head(10)
# 4. Visualizziamo il risultato
print(top_10_results)
# 3. Creiamo la classifica delle Top 10
# Ordiniamo per AUC (la metrica più robusta) e poi per Sensitivity
top_10_results <- summary_df %>%
arrange(desc(spec), desc(sens)) %>%
head(10)
# 4. Visualizziamo il risultato
print(top_10_results)
# 3. Creiamo la classifica delle Top 10
# Ordiniamo per AUC (la metrica più robusta) e poi per Sensitivity
top_10_results <- summary_df %>%
arrange(desc(auc), desc(spec)) %>%
head(10)
# 4. Visualizziamo il risultato
print(top_10_results)
# Definiamo il pool di componenti basandoci sull'evidenza dei tuoi modelli precendenti
pc_pool <- paste0("PC", 1:8)
# Generiamo tutte le combinazioni da 2 a 6 elementi. Questo produrrà esattamente 238 combinazioni
min_pc <- 2
max_pc <- 6
combinations_list <- map(min_pc:max_pc, function(k) {
combs <- combn(pc_pool, k, simplify = FALSE)
combs
}) |> flatten()
print(paste("Totale combinazioni da testare:", length(combinations_list)))
results_list <- lapply(combinations_list, function(x) {
model_qda_deep_dive(task, resampling_instance =  resampling.instance, pca_fix = TRUE, pca_selected = x)
})
extract_metrics <- function(res_item, index) {
if (is.null(res_item)) return(NULL)
current_pcs <- paste(combinations_list[[index]], collapse = ", ")
data.frame(
pcs = current_pcs,
acc = res_item$performance["classif.acc"],
auc = res_item$performance["classif.auc"],
sens = res_item$performance["classif.sensitivity"],
spec = res_item$performance["classif.specificity"],
stringsAsFactors = FALSE
)
}
summary_df <- imap_dfr(results_list, extract_metrics)
top_10_results <- summary_df |> arrange(desc(auc), desc(spec)) |> head(10)
print(top_10_results)
extract_metrics <- function(res_item, index) {
if (is.null(res_item)) return(NULL)
current_pcs <- paste(combinations_list[[index]], collapse = ", ")
df <- data.frame(
pcs = current_pcs,
acc = res_item$performance["classif.acc"],
auc = res_item$performance["classif.auc"],
sens = res_item$performance["classif.sensitivity"],
spec = res_item$performance["classif.specificity"],
stringsAsFactors = FALSE
)
rownames(df) <- index
}
summary_df <- imap_dfr(results_list, extract_metrics)
summary_df <- imap_dfr(results_list, extract_metrics)
extract_metrics <- function(res_item, index) {
if (is.null(res_item)) return(NULL)
current_pcs <- paste(combinations_list[[index]], collapse = ", ")
df <- data.frame(
rank = index,
pcs = current_pcs,
acc = res_item$performance["classif.acc"],
auc = res_item$performance["classif.auc"],
sens = res_item$performance["classif.sensitivity"],
spec = res_item$performance["classif.specificity"],
stringsAsFactors = FALSE
)
}
top_10_results <- summary_df |> arrange(desc(auc), desc(spec)) |> head(10)
print(top_10_results)
summary_df <- imap_dfr(results_list, extract_metrics)
top_10_results <- summary_df |> arrange(desc(auc), desc(spec)) |> head(10)
print(top_10_results)
rownames(top_10_results) <- top_10_results$rank
top_10_results |> dplyr::select(!rank)
print(top_10_results)
y <- "K value"
paste0(y, sep = "")
paste0(y, sep = " ")
paste0(y, collapse = TRUE)
paste0(y, collapse = "")
paste0(y, collapse = " ")
paste(y)
paste0(y, collapse = "")
strsplit(y, split = "")
strsplit(y, split = " ")
a <- strsplit(y, split = " ")
paste0(a, collapse = "")
b <- paste0(a, collapse = "")
ptiny(b)
print(b)
a <- strsplit(y, split = " ")[[1]]
b <- paste0(a, collapse = "")
print(b)
y <- "day"
strsplit(y, split = " ")[[1]]
a <- strsplit(y, split = " ")[[1]]
b <- paste0(a, collapse = "")
