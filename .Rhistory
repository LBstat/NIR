panel.grid.minor = element_blank())
p3 <- autoplot(results1$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
c((p1 | p2) / p3)
autoplot(results1$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC satbility between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results1$performance
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
head(summary(pca_res)$importance[3, ], n = 12)
pca_res <- prcomp(data_cluster_final[, -141], center = TRUE, scale. = TRUE)
source("settings.R")
source("source_all.R")
if (!file.exists("data/intermediate/data analysis.rds")) {
build_data_main()
}
data_analysis <- readRDS("data/intermediate/data analysis.rds")
knitr::include_graphics("plot/densities1.png")
knitr::include_graphics("plot/densities2.png")
pattern <- "^[[:digit:]]*$"
data_for_imputation <- data_analysis |> dplyr::select(!c(grep(pattern, colnames(data_analysis)), sample))
flextable(as.data.frame(cor(data_for_imputation, use = "complete.obs")))
m <- 10
imp <- mice(data_for_imputation[, -7], m = m, maxit = 50, seed = 123, method = "pmm", printFlag = FALSE)
imputed_Kvalues <- lapply(seq_len(m), function(i) {
df <- complete(imp, i)
df$`K value` <- calculate_kvalue(df)
})
imputed_pos <- which(is.na(data_for_imputation$`K value`))
real_vals <- data_for_imputation$`K value`[-imputed_pos]
ks_distances <- vapply(imputed_Kvalues, function(x) {
imputed_vals <- x[imputed_pos]
ks.test(real_vals, imputed_vals)$statistic
}, numeric(1))
best_idx <- which.min(unname(ks.distances))
cat(paste0("Statistical Distribution (KS-Test): Dataset ", best_idx, " is the most faithful to the original data shape."))
trend_errors <- vapply(imputed_Kvalues, function(x) {
df_temp <- data.frame(k = x, day = data_for_imputation$day)
mod <- lm(k ~ day, data = df_temp[-imputed_pos, ])
preds <- predict(mod, newdata = df_temp[imputed_pos, ])
mean((x[imputed_pos] - preds)^2)
}, numeric(1))
best_idx <- which.min(trend_errors)
cat(paste0("Biological Trend (MSE-Linear): Dataset ", best_idx, " is the most consistent with the degradation over time."))
complete_data <- complete(imp, "long")
flextable(as.data.frame(cor(complete_data[, -c(14, 15)], use = "complete.obs")))
flextable(as.data.frame(cor(data_forimputation[, -7], use = "complete.obs")))
pattern <- "^[[:digit:]]*$"
data_for_imputation <- data_analysis |> dplyr::select(!c(grep(pattern, colnames(data_analysis)), sample))
flextable(as.data.frame(cor(data_for_imputation, use = "complete.obs")))
m <- 10
imp <- mice(data_for_imputation[, -7], m = m, maxit = 50, seed = 123, method = "pmm", printFlag = FALSE)
imputed_Kvalues <- lapply(seq_len(m), function(i) {
df <- complete(imp, i)
df$`K value` <- calculate_kvalue(df)
})
imputed_pos <- which(is.na(data_for_imputation$`K value`))
real_vals <- data_for_imputation$`K value`[-imputed_pos]
ks_distances <- vapply(imputed_Kvalues, function(x) {
imputed_vals <- x[imputed_pos]
ks.test(real_vals, imputed_vals)$statistic
}, numeric(1))
best_idx <- which.min(unname(ks.distances))
cat(paste0("Statistical Distribution (KS-Test): Dataset ", best_idx, " is the most faithful to the original data shape."))
trend_errors <- vapply(imputed_Kvalues, function(x) {
df_temp <- data.frame(k = x, day = data_for_imputation$day)
mod <- lm(k ~ day, data = df_temp[-imputed_pos, ])
preds <- predict(mod, newdata = df_temp[imputed_pos, ])
mean((x[imputed_pos] - preds)^2)
}, numeric(1))
best_idx <- which.min(trend_errors)
cat(paste0("Biological Trend (MSE-Linear): Dataset ", best_idx, " is the most consistent with the degradation over time."))
complete_data <- complete(imp, "long")
flextable(as.data.frame(cor(complete_data[, -c(14, 15)], use = "complete.obs")))
flextable(as.data.frame(cor(data_for_imputation[, -7], use = "complete.obs")))
imputed_df <- lapply(seq_len(m), function(i) {
df <- complete(imp, i)
df$`K value` <- calculate_kvalue(df)
df
})
graphics <- lapply(seq_along(imputed.df), function(i) {
graphical_comparation(imputed.df[[i]], imputed_pos = imputed_pos, x = "day", y = "K value", x_ignore = TRUE, dataset_id = i)
})
knitr::include_graphics("plot/imputation_2_Kvalue.png")
knitr::include_graphics("plot/imputation_5_Kvalue.png")
data_imputated <- imputed_df[[5]]
data_imputated$class <- ifelse(data_imputated$`K value` < 0.2, "very fresh",
ifelse(data_imputated$`K value` <= 0.5, "good/acceptable", "impending spoilage/spoiled"))
data_imputated$class <- as.factor(data_imputated$class)
table(data_imputated$class)
ggplot(data_imputated, aes(x = `K value`, fill = class)) +
geom_histogram(position = "identity", alpha = 0.8, bins = 30) +
scale_fill_manual(values = c("good/acceptable" = "#003366", "impending spoilage/spoiled" = "#9B1B30")) +
labs(
title = "Histogram of K value distribution by class",
x = "K value",
y = "Frequency"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
data_cluster <- data_analysis |> dplyr::select(grep("^[[:digit:]]*$", colnames(data_analysis), value = TRUE))
colnames(data_cluster) <- paste0("V", colnames(data_cluster))
data_cluster_smooth <- smoothing(data_cluster)
data_cluster_snv <- as_tibble(t(apply(data_cluster_smooth, MARGIN = 1, FUN = normalization)))
plot_spectral_comparison(data_cluster, data_cluster_snv, n_samples = 6)
knitr::include_graphics("plot/spectral_smoothing_comparison.png")
data_cluster_final <- bind_cols(data_cluster_snv, class = data_imputated$class)
task <- TaskClassif$new(
id = "Classification original data",
backend = data_cluster_final,
target = "class",
positive = "good/acceptable"
)
task$col_roles$stratum <- "class"
methods <- c("anova", "auc", "mrmr")
tab <- as.data.frame(lapply(methods, function(i) feature_selection(task, i)))
colnames(tab) <- methods
head(tab)
set.seed(123)
resampling_outer <- rsmp("cv", folds = 5)
resampling_instance <- resampling_outer$instantiate(task)
if (!file.exists("data/results/results1.rds")) {
feature_analysis(task, resampling_instance = resampling_instance)
}
results1 <- readRDS("data/results/results1.rds")
results1$performance
autoplot(results1$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC satbility between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results1$learners$LDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$QDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$kNN$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$SVM$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$confusion$QDA
results1$learners$QDA$learner$model$classif.qda$model
preds <- as.data.table(results1$resampling$QDA$prediction())
preds[, .(row_ids, truth, response,`prob.good/acceptable` = round(`prob.good/acceptable`, digits = 4), `prob.impending spoilage/spoiled` = round(`prob.impending spoilage/spoiled`, digits = 4))]
false_positive_idx <- preds[truth == "impending spoilage/spoiled" & response == "good/acceptable", row_ids]
data_wrong <- data_imputated[false_positive_idx, ]
errors <- data.frame(
row_ID = false_positive_idx,
day = data_wrong$day
)
cat("Spoiled samples classified as Fresh (false positives)")
print(errors)
data_imputated[errors$row_ID,]$`K value`
data_imputated$class <- ifelse(data_imputated$`K value` < 0.2, "very fresh",
ifelse(data_imputated$`K value` <= 0.4, "good/acceptable", "impending spoilage/spoiled"))
data_cluster_final <- data_cluster_final |> dplyr::select(!class)
data_cluster_final <- bind_cols(data_cluster_final, class = as.factor(data.imputated$class))
table(data_cluster_final$class)
task <- TaskClassif$new(
id = "Classification original data",
backend = data_cluster_final,
target = "class",
positive = "good/acceptable"
)
table(data_cluster_final)
data_imputated$class <- ifelse(data_imputated$`K value` < 0.2, "very fresh",
ifelse(data_imputated$`K value` <= 0.4, "good/acceptable", "impending spoilage/spoiled"))
data_cluster_final <- data_cluster_final |> dplyr::select(!class)
data_cluster <- data_analysis |> dplyr::select(grep("^[[:digit:]]*$", colnames(data_analysis), value = TRUE))
colnames(data_cluster) <- paste0("V", colnames(data_cluster))
data_cluster_smooth <- smoothing(data_cluster)
data_cluster_snv <- as_tibble(t(apply(data_cluster_smooth, MARGIN = 1, FUN = normalization)))
plot_spectral_comparison(data_cluster, data_cluster_snv, n_samples = 6)
knitr::include_graphics("plot/spectral_smoothing_comparison.png")
data_cluster_final <- bind_cols(data_cluster_snv, class = data_imputated$class)
task <- TaskClassif$new(
id = "Classification original data",
backend = data_cluster_final,
target = "class",
positive = "good/acceptable"
)
data_cluster <- data_analysis |> dplyr::select(grep("^[[:digit:]]*$", colnames(data_analysis), value = TRUE))
colnames(data_cluster) <- paste0("V", colnames(data_cluster))
data_cluster_smooth <- smoothing(data_cluster)
data_cluster_snv <- as_tibble(t(apply(data_cluster_smooth, MARGIN = 1, FUN = normalization)))
plot_spectral_comparison(data_cluster, data_cluster_snv, n_samples = 6)
knitr::include_graphics("plot/spectral_smoothing_comparison.png")
data_cluster_final <- bind_cols(data_cluster_snv, class = as.factor(data_imputated$class))
task <- TaskClassif$new(
id = "Classification original data",
backend = data_cluster_final,
target = "class",
positive = "good/acceptable"
)
task$col_roles$stratum <- "class"
methods <- c("anova", "auc", "mrmr")
tab <- as.data.frame(lapply(methods, function(i) feature_selection(task, i)))
colnames(tab) <- methods
head(tab)
set.seed(123)
resampling_outer <- rsmp("cv", folds = 5)
resampling_instance <- resampling_outer$instantiate(task)
if (!file.exists("data/results/results1.rds")) {
feature_analysis(task, resampling_instance = resampling_instance)
}
results1 <- readRDS("data/results/results1.rds")
results1$performance
autoplot(results1$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC satbility between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results1$learners$LDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$QDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$kNN$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$SVM$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$confusion$QDA
results1$learners$QDA$learner$model$classif.qda$model
preds <- as.data.table(results1$resampling$QDA$prediction())
preds[, .(row_ids, truth, response,`prob.good/acceptable` = round(`prob.good/acceptable`, digits = 4), `prob.impending spoilage/spoiled` = round(`prob.impending spoilage/spoiled`, digits = 4))]
false_positive_idx <- preds[truth == "impending spoilage/spoiled" & response == "good/acceptable", row_ids]
data_wrong <- data_imputated[false_positive_idx, ]
errors <- data.frame(
row_ID = false_positive_idx,
day = data_wrong$day
)
cat("Spoiled samples classified as Fresh (false positives)")
print(errors)
data_imputated[errors$row_ID,]$`K value`
data_imputated$class <- ifelse(data_imputated$`K value` < 0.2, "very fresh",
ifelse(data_imputated$`K value` <= 0.4, "good/acceptable", "impending spoilage/spoiled"))
data_cluster_final <- data_cluster_final |> dplyr::select(!class)
data_cluster_final <- bind_cols(data_cluster_final, class = as.factor(data_imputated$class))
table(data_cluster_final$class)
task <- TaskClassif$new(
id = "Classification original data",
backend = data_cluster_final,
target = "class",
positive = "good/acceptable"
)
task$col_roles$stratum <- "class"
methods <- c("anova", "auc", "mrmr")
tab <- as.data.frame(lapply(methods, function(i) feature_selection(task, i)))
colnames(tab) <- methods
head(tab, 10)
set.seed(123)
resampling_outer <- rsmp("cv", folds = 5)
resampling_instance <- resampling_outer$instantiate(task)
if (!file.exists("data/results/results1.1.rds")) {
feature_analysis(task, resampling_instance = resampling_instance, new_results = TRUE)
}
results1.1 <- readRDS("data/results/results1.1.rds")
results1.1$performance
autoplot(results1.1$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC satbility between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1.1$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1.1$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results1.1$learners$LDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1.1$learners$QDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1.1$learners$kNN$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1.1$learners$SVM$learner$graph_model$pipeops$feat_select$state$outtasklayout
preds <- as.data.table(results1.1$resampling$QDA$prediction())
preds[, .(row_ids, truth, response,`prob.good/acceptable` = round(`prob.good/acceptable`, digits = 4), `prob.impending spoilage/spoiled` = round(`prob.impending spoilage/spoiled`, digits = 4))]
false_positive_idx <- preds[truth == "impending spoilage/spoiled" & response == "good/acceptable", row_ids]
data_wrong <- data_imputated[false_positive_idx, ]
errors <- data.frame(
row_ID = false_positive_idx,
day = data_wrong$day
)
cat("Spoiled samples classified as Fresh (false positives)")
print(errors)
data_imputated[errors$row_ID,]$`K value`
pca_res <- prcomp(data_cluster_final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca_res)$importance[3, ], n = 12)
pca_representation_facet(pca_res, as.factor(data_cluster_final$class), 6)
if (!file.exists("data/results/results2.rds")) {
feature_analysis(task, resampling_instance = resampling_instance, filter = FALSE, pca = TRUE)
}
results2 <- readRDS("data/results/results2.rds")
results2$performance
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
pc_pool <- paste0("PC", 1:8)
min_pc <- 2
max_pc <- 6
combinations_list <- map(min_pc:max_pc, function(k) {
combs <- combn(pc_pool, k, simplify = FALSE)
combs
}) |> flatten()
print(paste("Total number of possible combinations", length(combinations_list)))
if (!file.exists("data/results/results_list.rds")) {
results_list <- lapply(combinations_list, function(x) {
model_qda_deep_dive(task, resampling_instance =  resampling.instance, pca_fix = TRUE, pca_selected = x)
})
saveRDS(results_list, file = "data/results/results_list.rds")
}
results_list <- readRDS("data/results/results_list.rds")
summary_df <- imap_dfr(results_list, extract_metrics)
top_10_results <- summary_df |> arrange(desc(auc), desc(spec)) |> head(10)
rownames(top_10_results) <- top_10_results$rank
top_10_results |> dplyr::select(!rank)
top_10_results
if (!file.exists("data/results/results_final.rds")) {
result_final<- model_qda_deep_dive(task, resampling_instance = resampling.instance, pca_rank = 12, pca_fix = TRUE, pca_selected = c("PC1", "PC2", "PC3", "PC4", "PC6", "PC7"))
saveRDS(result_final, file = "data/results/results_final.rds")
}
result_final <- readRDS("data/results/results_final.rds")
result_final$performance
result_final$confusion_matrix
bmr <- as_benchmark_result(result_final$resampling_results)
autoplot(bmr, measure = msr("classif.auc")) +
labs(title = "AUC stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(bmr, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(bmr, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
preds <- as.data.table(results_final$resampling_results$prediction())
preds[, .(row_ids, truth, response,`prob.good/acceptable` = round(`prob.good/acceptable`, digits = 4), `prob.impending spoilage/spoiled` = round(`prob.impending spoilage/spoiled`, digits = 4))]
false_positive_idx <- preds[truth == "impending spoilage/spoiled" & response == "good/acceptable", row_ids]
data_wrong <- data_imputated[false_positive_idx, ]
errors <- data.frame(
row_ID = false_positive_idx,
day = data_wrong$day
)
cat("Spoiled samples classified as Fresh (false positives)")
print(errors)
data_imputated[errors$row_ID,]$`K value`
head(summary(pca_res)$importance[3, ], n = 12)
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$performance
result_final$performance
results2$performance
result_final$performance
results2$performance
result_final$performance
pca_res$rotation
pca_res$rotation[,1:12]
pca_res$rotation[,1:12]
which.max(pca_res$rotation[,1])
pca_res$rotation[49,1]
which.max(abs(pca_res$rotation[,1]))
pca_res$rotation[81,1]
which.max(abs(pca_res$rotation[,1]))
which.max(abs(pca_res$rotation[,2]))
which.max(abs(pca_res$rotation[,3]))
which.max(abs(pca_res$rotation[,4]))
which.max(abs(pca_res$rotation[,5]))
which.max(abs(pca_res$rotation[,6]))
which.max(abs(pca_res$rotation[,7]))
pca_res$rotation[23,7]
# Esempio concettuale per estrarre i loadings dal modello finale
pca_obj <- result_final$final_model_object$model$rank_select$state$model
loadings_pc7 <- pca_obj$rotation[, "PC7"]
plot(loadings_pc7, type = "l", main = "Loadings PC7")
pca_obj <- result_final$final_model_object$model$rank_select$state$model
> loadings_pc7 <- pca_obj$rotation[, "PC7"]
pca_obj <- result_final$final_model_object$model$rank_select$state$model
loadings_pc7 <- pca_obj$rotation[, "PC7"]
plot(loadings_pc7, type = "l", main = "Loadings PC7")
# Verifica se ci sono valori finiti
summary(loadings_pc7)
# Verifica se il vettore Ã¨ vuoto o contiene solo NA
any(is.finite(loadings_pc7))
# Supponiamo che 'loadings_pc7' sia il tuo vettore
# Creiamo un indice per l'asse X (es. da 1 a numero di variabili)
x_axis <- 1:length(loadings_pc7)
# Filtriamo per sicurezza solo i valori finiti
valid_indices <- is.finite(loadings_pc7)
# Lanciamo il plot specificando i limiti in modo sicuro
plot(x_axis[valid_indices], loadings_pc7[valid_indices],
type = "l",
xlab = "Variabili (Lunghezze d'onda)",
ylab = "Loading PC7",
main = "Profilo dei Loadings PC7")
pca_obj <- result_final$final_model_object$model$pca_gen
pca_obj$rotation
loadings_pc7 <- pca_obj$rotation[, "PC7"]plot(loadings_pc7, type = "l", main = "Loadings PC7")
loadings_pc7 <- pca_obj$rotation[, "PC7"]
plot(loadings_pc7, type = "l", main = "Loadings PC7")
