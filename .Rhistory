plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
imputed.df <- lapply(seq_len(m), function(i) {
df <- complete(imp, i)
df$`K value` <- calculate.kvalue(df)
df
})
graphics <- lapply(seq_along(imputed.df), function(i) {
graphical.comparation(imputed.df[[i]], imputed_pos = imputed.pos, x = "day", y = "K value", x_ignore = TRUE, dataset_id = i)
})
knitr::include_graphics("plot/imputation_2_Kvalue.png")
knitr::include_graphics("plot/imputation_5_Kvalue.png")
data.imputated <- imputed.df[[5]]
data.imputated$class <- ifelse(data.imputated$`K value` < 0.2, "very fresh",
ifelse(data.imputated$`K value` <= 0.5, "good/acceptable",
ifelse(data.imputated$`K value` <= 0.7, "impending spoilage", "spoiled")))
data.imputated$class <- as.factor(data.imputated$class)
ggplot(data.imputated, aes(x = `K value`, fill = class)) +
geom_histogram(position = "identity", alpha = 0.8, bins = 30) +
scale_fill_manual(values = c("good/acceptable" = "#2ECC71", "impending spoilage" = "#003366", "spoiled" = "#9B1B30")) +
labs(
title = "Histogram of K value distribution by class",
x = "K value",
y = "Frequency"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
ggplot(data.imputated, aes(x = `K value`, fill = class)) +
geom_histogram(position = "identity", alpha = 0.8, bins = 30) +
scale_fill_manual(values = c("good/acceptable" = "#2ECC71", "impending spoilage" = "#003366", "spoiled" = "#9B1B30")) +
labs(
title = "Histogram of K value distribution by class",
x = "K value",
y = "Frequency"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
table(data.imputated$class)
imputed.df <- lapply(seq_len(m), function(i) {
df <- complete(imp, i)
df$`K value` <- calculate.kvalue(df)
df
})
graphics <- lapply(seq_along(imputed.df), function(i) {
graphical.comparation(imputed.df[[i]], imputed_pos = imputed.pos, x = "day", y = "K value", x_ignore = TRUE, dataset_id = i)
})
knitr::include_graphics("plot/imputation_2_Kvalue.png")
knitr::include_graphics("plot/imputation_5_Kvalue.png")
data.imputated <- imputed.df[[5]]
data.imputated$class <- ifelse(data.imputated$`K value` < 0.2, "very fresh",
ifelse(data.imputated$`K value` <= 0.5, "good/acceptable", "impending spoilage / spoiled"))
data.imputated$class <- as.factor(data.imputated$class)
ggplot(data.imputated, aes(x = `K value`, fill = class)) +
geom_histogram(position = "identity", alpha = 0.8, bins = 30) +
scale_fill_manual(values = c("good/acceptable" = "#003366", "impending spoilage / spoiled" = "#9B1B30")) +
labs(
title = "Histogram of K value distribution by class",
x = "K value",
y = "Frequency"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
ggplot(data.imputated, aes(x = `K value`, fill = class)) +
geom_histogram(position = "identity", alpha = 0.8, bins = 30) +
scale_fill_manual(values = c("good/acceptable" = "#003366", "impending spoilage / spoiled" = "#9B1B30")) +
labs(
title = "Histogram of K value distribution by class",
x = "K value",
y = "Frequency"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
data.cluster <- data.analysis |> dplyr::select(grep("^[[:digit:]]*$", colnames(data.analysis), value = TRUE))
colnames(data.cluster) <- paste0("V", colnames(data.cluster))
data.cluster.smooth <- smoothing(data.cluster)
data.cluster.snv <- as_tibble(t(apply(data.cluster.smooth, MARGIN = 1, FUN = normalization)))
plot_spectral_comparison(data.cluster, data.cluster.snv, n_samples = 6)
knitr::include_graphics("plot/spectral_smoothing_comparison.png")
data.cluster.final <- bind_cols(data.cluster.snv, class = data.imputated$class)
table(data.cluster.final$class)
task <- TaskClassif$new(
id = "Classification original data",
backend = data.cluster.final,
target = "class",
positive = "good / acceptable"
)
task <- TaskClassif$new(
id = "Classification original data",
backend = data.cluster.final,
target = "class",
positive = "good/acceptable"
)
task$col_roles$stratum <- "class"
methods <- c("anova", "auc", "mrmr")
tab <- as.data.frame(lapply(methods, function(i) feature_selection(task, i)))
colnames(tab) <- methods
head(tab, 10)
set.seed(123)
resampling.outer <- rsmp("cv", folds = 5)
resampling.instance <- resampling.outer$instantiate(task)
if (!file.exists("data/results/results1.rds")) {
feature_analysis(task, resampling_instance = resampling.instance)
}
View(data.imputated)
View(data.imputated)
View(data.imputated)
View(data.imputated)
View(data.imputated)
View(data.imputated)
results1 <- readRDS("data/results/results1.rds")
results1$performance
autoplot(results1$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC satbility between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results1$learners$LDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$QDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$kNN$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$SVM$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$QDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$LDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$kNN$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$SVM$learner$graph_model$pipeops$feat_select$state$outtasklayout
task <- TaskClassif$new(
id = "Classification original data",
backend = data.cluster.final,
target = "class",
positive = "good/acceptable"
)
task$col_roles$stratum <- "class"
methods <- c("anova", "auc", "mrmr")
tab <- as.data.frame(lapply(methods, function(i) feature_selection(task, i)))
colnames(tab) <- methods
head(tab, 10)
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, data.analysis$class, 6)
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, as.factor(data.analysis$class), 6)
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, as.factor(data.analysis$class), 6)
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, as.factor(data.cluster.final$class), 6)
if (!file.exists("data/results/results2.rds")) {
feature_analysis(task, resampling_instance = resampling.instance, filter = FALSE, pca = TRUE)
}
source("~/Desktop/NIR/code/plot.R")
results2 <- readRDS("data/results/results2.rds")
results2$performance
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
task
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, as.factor(data.cluster.final$class), 6)
if (!file.exists("data/results/results2.rds")) {
feature_analysis(task, resampling_instance = resampling.instance, filter = FALSE, pca = TRUE)
}
results2 <- readRDS("data/results/results2.rds")
results2$performance
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
source("~/Desktop/NIR/code/plot.R")
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, as.factor(data.cluster.final$class), 6)
if (!file.exists("data/results/results2.rds")) {
feature_analysis(task, resampling_instance = resampling.instance, filter = FALSE, pca = TRUE)
}
results2 <- readRDS("data/results/results2.rds")
results2$performance
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
data.imputated$class <- ifelse(data.imputated$`K value` < 0.2, "very fresh",
ifelse(data.imputated$`K value` <= 0.5, "good/acceptable", "impending spoilage/spoiled"))
data.imputated$class <- as.factor(data.imputated$class)
table(data.cluster.final$class)
ggplot(data.imputated, aes(x = `K value`, fill = class)) +
geom_histogram(position = "identity", alpha = 0.8, bins = 30) +
scale_fill_manual(values = c("good/acceptable" = "#003366", "impending spoilage/spoiled" = "#9B1B30")) +
labs(
title = "Histogram of K value distribution by class",
x = "K value",
y = "Frequency"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
data.cluster <- data.analysis |> dplyr::select(grep("^[[:digit:]]*$", colnames(data.analysis), value = TRUE))
colnames(data.cluster) <- paste0("V", colnames(data.cluster))
data.cluster.smooth <- smoothing(data.cluster)
data.cluster.snv <- as_tibble(t(apply(data.cluster.smooth, MARGIN = 1, FUN = normalization)))
plot_spectral_comparison(data.cluster, data.cluster.snv, n_samples = 6)
knitr::include_graphics("plot/spectral_smoothing_comparison.png")
data.cluster.final <- bind_cols(data.cluster.snv, class = data.imputated$class)
task <- TaskClassif$new(
id = "Classification original data",
backend = data.cluster.final,
target = "class",
positive = "good/acceptable"
)
task$col_roles$stratum <- "class"
methods <- c("anova", "auc", "mrmr")
tab <- as.data.frame(lapply(methods, function(i) feature_selection(task, i)))
colnames(tab) <- methods
head(tab, 10)
set.seed(123)
resampling.outer <- rsmp("cv", folds = 5)
resampling.instance <- resampling.outer$instantiate(task)
if (!file.exists("data/results/results1.rds")) {
feature_analysis(task, resampling_instance = resampling.instance)
}
results1 <- readRDS("data/results/results1.rds")
results1$performance
autoplot(results1$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC satbility between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results1$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results1$learners$LDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$QDA$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$kNN$learner$graph_model$pipeops$feat_select$state$outtasklayout
results1$learners$SVM$learner$graph_model$pipeops$feat_select$state$outtasklayout
pca.res <- prcomp(data.cluster.final[, -141], center = TRUE, scale. = TRUE)
head(summary(pca.res)$importance[3, ], n = 12)
pca_representation_facet(pca.res, as.factor(data.cluster.final$class), 6)
if (!file.exists("data/results/results2.rds")) {
feature_analysis(task, resampling_instance = resampling.instance, filter = FALSE, pca = TRUE)
}
results2 <- readRDS("data/results/results2.rds")
results2$performance
autoplot(results2$benchmark_obj, measure = msr("classif.auc")) +
labs(title = "AUC stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(results2$benchmark_obj, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results2$learners$LDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$QDA$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$kNN$learner$graph_model$pipeops$rank_select$state$outtasklayout
results2$learners$SVM$learner$graph_model$pipeops$rank_select$state$outtasklayout
# Definiamo il pool di componenti basandoci sull'evidenza dei tuoi modelli precendenti
pc_pool <- paste0("PC", 1:8)
# Generiamo tutte le combinazioni da 2 a 6 elementi. Questo produrrà esattamente 238 combinazioni
min_pc <- 2
max_pc <- 6
combinations_list <- map(min_pc:max_pc, function(k) {
combs <- combn(pc_pool, k, simplify = FALSE)
combs
}) |> flatten()
print(paste("Totale combinazioni da testare:", length(combinations_list)))
if (!file.exists("data/results/results_list.rds")) {
results_list <- lapply(combinations_list, function(x) {
model_qda_deep_dive(task, resampling_instance =  resampling.instance, pca_fix = TRUE, pca_selected = x)
})
saveRDS(results_list, file = "data/results/results_list.rds")
}
results_list <- readRDS("data/results/results_list.rds")
summary_df <- imap_dfr(results_list, extract_metrics)
top_10_results <- summary_df |> arrange(desc(auc), desc(spec)) |> head(10)
rownames(top_10_results) <- top_10_results$rank
top_10_results |> dplyr::select(!rank)
print(top_10_results)
if (!file.exists("data/results/results_final.rds")) {
result_final<- model_qda_deep_dive(task, resampling_instance = resampling.instance, pca_rank = 12, pca_fix = TRUE,
pca_selected = c("PC1","PC2", "PC3", "PC5"))
saveRDS(results_list, file = "data/results/results_final.rds")
}
results_final <- readRDS("data/results/results_final.rds")
results_final$performance
results_final$confusion_matrix
bmr <- as_benchmark_result(results_final$resampling_results)
autoplot(bmr, measure = msr("classif.auc")) +
labs(title = "AUC stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(bmr, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(bmr, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
top_10_results
top_10_results
# Definiamo il pool di componenti basandoci sull'evidenza dei tuoi modelli precendenti
pc_pool <- paste0("PC", 1:8)
# Generiamo tutte le combinazioni da 2 a 6 elementi. Questo produrrà esattamente 238 combinazioni
min_pc <- 2
max_pc <- 6
combinations_list <- map(min_pc:max_pc, function(k) {
combs <- combn(pc_pool, k, simplify = FALSE)
combs
}) |> flatten()
print(paste("Totale combinazioni da testare:", length(combinations_list)))
if (!file.exists("data/results/results_list.rds")) {
results_list <- lapply(combinations_list, function(x) {
model_qda_deep_dive(task, resampling_instance =  resampling.instance, pca_fix = TRUE, pca_selected = x)
})
saveRDS(results_list, file = "data/results/results_list.rds")
}
results_list <- readRDS("data/results/results_list.rds")
summary_df <- imap_dfr(results_list, extract_metrics)
top_10_results <- summary_df |> arrange(desc(auc), desc(spec)) |> head(10)
rownames(top_10_results) <- top_10_results$rank
top_10_results |> dplyr::select(!rank)
print(top_10_results)
if (!file.exists("data/results/results_final.rds")) {
result_final<- model_qda_deep_dive(task, resampling_instance = resampling.instance, pca_rank = 12, pca_fix = TRUE,
pca_selected = c("PC2","PC3", "PC5", "PC8"))
saveRDS(results_list, file = "data/results/results_final.rds")
}
results_final <- readRDS("data/results/results_final.rds")
results_final$performance
results_final$confusion_matrix
bmr <- as_benchmark_result(results_final$resampling_results)
autoplot(bmr, measure = msr("classif.auc")) +
labs(title = "AUC stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(bmr, measure = msr("classif.acc")) +
labs(title = "Accuracy stability between folds") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
autoplot(bmr, type = "roc") +
labs(title = "ROC curves (mean between folds)") +
theme_minimal() +
theme(
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
panel.grid.major = element_line(color = "lightgray", linewidth = 0.5),
panel.grid.minor = element_blank())
results_final$performance
results_final
